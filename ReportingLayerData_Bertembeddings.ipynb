{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5f9e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Using for cleaning and Pre-Processing\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "# Loading transformers library\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertModel,AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('Dhanush66/AntismetisimLargedata-finetuned-MLM-NEW')\n",
    "model = BertModel.from_pretrained('Dhanush66/AntismetisimLargedata-finetuned-MLM-NEW')\n",
    "# To generate embedding\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS,Phraser\n",
    "from gensim.models import Word2Vec, KeyedVectors #To load the model\n",
    "from cleantext import clean\n",
    "#Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Calibri\"\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from preprocesss import preprocess_batch\n",
    "#To check for performance\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keybert import KeyBERT\n",
    "from pygtrie import CharTrie\n",
    "from collections import Counter\n",
    "stopword=list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a690334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "667"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"Unmasking Antisemitism SRI Data Set - Reporting Layer.csv\")\n",
    "data=data[['Term or Phrase','Post Text']]\n",
    "data['Term or Phrase'].unique()\n",
    "\n",
    "#Get the number of rows\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c1a917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the duplicates\n",
    "data=data.drop_duplicates()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d8921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary=['cabal','cosmopolitan elite','cultural marxism','deicide','holocough','jewish capitalist','the goyim know',\n",
    "           'jewish communist','jewish lobby','new world order','rothschild', 'soros','zionist',\n",
    "         'zionist occupied government','jew down','not the real jews'] \n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                                \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a43b2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[-data[\"Post Text\"].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fcd12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lematizer=WordNetLemmatizer()\n",
    "def lematize(text):\n",
    "    text=text.split()\n",
    "    lema=[]\n",
    "    for i in text:\n",
    "        lema.append(lematizer.lemmatize(i))\n",
    "    return (\" \".join(lema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1163cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean']=data['Post Text'].apply(lambda x:x.lower())\n",
    "#removing the links:\n",
    "data['clean']=data['clean'].apply(lambda x:re.sub(r\"http\\S+\",\"\",str(x)))\n",
    "#Getting the lematised text to get the original form of the word\n",
    "data['lematize']=data['clean'].apply(lambda x:lematize(x))\n",
    "#data['clean']=data['clean'].apply(lambda x:x.translate(str.maketrans(\"\",\"\",string.punctuation)))\n",
    "uncleaned_texts=list(data[\"clean\"])\n",
    "uncleaned_lematised_text=list(data['lematize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135827ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text,postag,stopword,lematizer):\n",
    "    pos_removal=[tag[0] for tag in pos_tag(text.split()) if tag[1] in postag]\n",
    "    return(\" \".join([lematizer.lemmatize(i) for i in pos_removal if i not in stopword]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7625bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "postag=[\"JJ\",'NN','NNS','NNP','VBP']\n",
    "stopword=list(stopwords.words('english'))\n",
    "def preprocess_parallel(sentences, postag, stopword, num_workers):\n",
    "    chunk_size = len(sentences) // num_workers\n",
    "    chunks = [sentences[i:i + chunk_size] for i in range(0, len(sentences)+1, chunk_size)]\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = executor.map(preprocess_batch, chunks, [postag] * num_workers, [stopword] * num_workers)\n",
    "    processed_sentences = [item for sublist in results for item in sublist]\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a04d0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences_parallel = preprocess_parallel(sentences=uncleaned_texts, \n",
    "                                                   postag=postag, stopword=stopword, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ad17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean\"]=processed_sentences_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9729b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lematize']=data['clean'].apply(lambda x:lematize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "191c605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_text=list(data['Post Text'])\n",
    "\n",
    "lematised_texts=list(data['lematize'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2145a6e",
   "metadata": {},
   "source": [
    "## Finding Important terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7bf30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emerging(terms):\n",
    "\n",
    "    check=True\n",
    "    while(tqdm(check)):\n",
    "        #For each time step\n",
    "        important_terms={}\n",
    "        text=' '.join(data.lematize)\n",
    "        #Generate TF-IDF Matrix\n",
    "        tf_idfvectorizer=TfidfVectorizer(ngram_range=(2,3))\n",
    "        tfidf=tf_idfvectorizer.fit_transform(data.lematize)\n",
    "        features=tf_idfvectorizer.get_feature_names_out()\n",
    "        \n",
    "        #Exporting all Bigrams\n",
    "        #pd.DataFrame(features).to_csv(\"Bigrams \"+str(z)+\".csv\",header=['All Bigrams'])\n",
    "        \n",
    "        #Finding Emerging terms/Initializing glossary\n",
    "        emerging_terms=list(set(features).difference(glossary))\n",
    "\n",
    "        emerging_terms_trie = CharTrie()\n",
    "        for term in emerging_terms:\n",
    "            emerging_terms_trie[term] = True\n",
    "\n",
    "                #glossary.append(i)\n",
    "        print(\"started\")\n",
    "        #Finding the index of emerging terms\n",
    "        findex=[i for i, word in enumerate(features) if word in emerging_terms_trie]\n",
    "        \n",
    "        tfidf_highest=tfidf.max(axis=0)\n",
    "        #Finding the highest tf-idf value for all the feature names across all documents\n",
    "        tfidf_values={}\n",
    "        for i in tqdm(findex):\n",
    "            tfidf_values[features[i]]=tfidf_highest.toarray()[0][i]\n",
    "        \n",
    "        tfidf_values=sorted(tfidf_values.items(),key=lambda x:x[1],reverse=True)\n",
    "        print(\"Ended\")\n",
    "        #Exporting Top Tf-idf values with bigrams\n",
    "        \n",
    "        #pd.DataFrame(tfidf_values).to_csv(\"SubtractionBigram-Tfidf \"+str(z)+\".csv\",header  = ['Bigrams','TF-IDF Values'])\n",
    "        #print(tfidf_values[:20])\n",
    "    \n",
    "        #Extracting bigrams after TF-IDF threshold cut off\n",
    "        a=[j for i, j in tfidf_values]\n",
    "        th=np.mean(a)\n",
    "        print(\"Threshold \",th)\n",
    "        final_tfidf={}\n",
    "        for i in tfidf_values:\n",
    "            if i[1]>th and len(i[0].split()[0])<20 and len(i[0].split()[1])<20:\n",
    "                final_tfidf[i[0]]=i[1]\n",
    "        #Exporting Tf-idf values above threshold\n",
    "        #pd.DataFrame.from_dict(data=final_tfidf, orient='index').to_csv(\"Bigram_threshold \"+str(z)+\".csv\",index_label='Bigrams',header  = ['TF-IDF Values'])\n",
    "        \n",
    "        #Frequency of words in that window size\n",
    "\n",
    "\n",
    "        words_frequency={}\n",
    "        text1=text.split(\" \")\n",
    "        pairs=list(zip(text1[:-1],text1[1:]))\n",
    "        trigram_pairs=list(zip(text1[:-2],text1[1:-1],text1[2:]))\n",
    "        bigrams=[' '.join(i) for i in pairs]\n",
    "        trigrams=[\" \".join(i) for i in trigram_pairs]\n",
    "        bigram_frequency=Counter(bigrams)\n",
    "        trigram_frequency=Counter(trigrams)\n",
    "        for word in (final_tfidf.keys()):\n",
    "            if len(word.split())==2:\n",
    "                words_frequency[word]=bigram_frequency[word]\n",
    "            else:\n",
    "                words_frequency[word]=trigram_frequency[word]\n",
    "                \n",
    "        words_frequency=sorted(words_frequency.items(),key=lambda x:x[1],reverse=True)\n",
    "        \n",
    "        #Appending terms in Imp terms list\n",
    "        for i in range(terms):\n",
    "            important_terms[words_frequency[i][0]] = words_frequency[i][1]\n",
    "        \n",
    "        \n",
    "        # Exporting Bigrams with frequency\n",
    "        #pd.DataFrame(words_frequency).to_csv(\"Bigram_frequency \"+str(z)+\".csv\",header  = ['Bigrams','Frequency'])\n",
    "        \n",
    "        #Exporting Important terms that is coming from  window\n",
    "        #pd.DataFrame.from_dict(data=important_terms, orient='index').to_csv(\"Important_terms \"+str(z)+\".csv\",index_label='Bigrams',header  = ['Frequency'])\n",
    "\n",
    "        return important_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b92663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61752/61752 [00:04<00:00, 12621.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended\n",
      "Threshold  0.07511590097790932\n"
     ]
    }
   ],
   "source": [
    "imp_terms=emerging(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daa7a027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new world': 68,\n",
       " 'world order': 42,\n",
       " 'zionist occupied': 32,\n",
       " 'united state': 26,\n",
       " 'goy know': 25,\n",
       " 'george soros': 22,\n",
       " 'blood type': 22,\n",
       " 'vatican ii': 22,\n",
       " 'nostra aetate': 21,\n",
       " 'occupied government': 17,\n",
       " 'human right': 16,\n",
       " 'jesus christ': 15,\n",
       " 'deep state': 13,\n",
       " 'zionist reptilian': 12,\n",
       " 'far right': 11,\n",
       " 'white people': 11,\n",
       " 'jewish community': 11,\n",
       " 'fascist white': 11,\n",
       " 'new york': 10,\n",
       " 'including zionist': 10,\n",
       " 'church always': 10,\n",
       " 'un special': 9,\n",
       " 'late 20th': 9,\n",
       " 'space time': 9,\n",
       " 'space time information': 9,\n",
       " 'time information': 9,\n",
       " 'white power': 9,\n",
       " 'african identity': 8,\n",
       " 'zionist jewish': 8,\n",
       " 'moon landing': 8,\n",
       " 'middle east': 8,\n",
       " 'critical race': 8,\n",
       " 'church teaching': 8,\n",
       " 'people color': 8,\n",
       " 'civil right': 8,\n",
       " 'world war': 7,\n",
       " 'old school': 7,\n",
       " 'zionist time': 7,\n",
       " 'special rapporteur': 7,\n",
       " 'un special rapporteur': 7,\n",
       " 'federal reserve': 7,\n",
       " 'white men': 7,\n",
       " 'interfere behalf': 7,\n",
       " 'say jew': 7,\n",
       " 'throw jew': 6,\n",
       " 'get rid': 6,\n",
       " 'need get': 6,\n",
       " 'zionist jew': 6,\n",
       " 'klaus schwab': 6,\n",
       " 'khazarian satanist': 6,\n",
       " 'mel gibson': 6,\n",
       " 'central bank': 6,\n",
       " 'sound like': 6,\n",
       " 'social justice': 6,\n",
       " 'american jew': 6,\n",
       " 'jewish lobby group': 6,\n",
       " 'lobby group': 6,\n",
       " 'published online': 6,\n",
       " 'always taught': 6,\n",
       " 'church always taught': 6,\n",
       " 'communist party': 6,\n",
       " 'ruling class': 6,\n",
       " 'jew well': 5,\n",
       " 'throw jew well': 5,\n",
       " 'real jew': 5,\n",
       " 'go back': 5,\n",
       " 'khazarian mafia': 5,\n",
       " 'white house': 5,\n",
       " 'jewish cabal': 5,\n",
       " 'national socialist': 5,\n",
       " 'destroying nation': 5,\n",
       " 'destroying entire': 5,\n",
       " 'full disclosure': 5,\n",
       " 'state america': 5,\n",
       " 'united state america': 5,\n",
       " 'military operation': 5,\n",
       " 'year ago': 5,\n",
       " 'air traffic': 5,\n",
       " 'public service': 5,\n",
       " 'white american': 5,\n",
       " 'german people': 5,\n",
       " '20th century': 5,\n",
       " 'white race': 5,\n",
       " 'jew accept': 5,\n",
       " 'jew reject': 5,\n",
       " 'organized jewry': 5,\n",
       " 'roy cohn': 4,\n",
       " 'elite killed': 4,\n",
       " 'hollywood elite': 4,\n",
       " 'hollywood elite killed': 4,\n",
       " 'globalists cabal': 4,\n",
       " 'world economic': 4,\n",
       " 'accursed guilty': 4,\n",
       " 'ethnic foids': 4,\n",
       " 'manhattan new': 4,\n",
       " 'manhattan new york': 4,\n",
       " 'new york city': 4,\n",
       " 'york city': 4,\n",
       " 'cultural marxist': 4,\n",
       " 'name zionist': 4,\n",
       " 'blame slav': 4,\n",
       " 'mayer amschel': 4,\n",
       " 'political correctness': 4,\n",
       " 'interest group': 4,\n",
       " 'african identity black': 4,\n",
       " 'identity black': 4,\n",
       " 'frankfurt school': 4,\n",
       " 'one listening': 4,\n",
       " 'nazi germany': 4,\n",
       " 'coup attempt': 4,\n",
       " 'rothschild family': 4,\n",
       " 'western civilization': 4,\n",
       " 'chairman jewish': 4,\n",
       " 'chairman jewish lobby': 4,\n",
       " 'anti white': 4,\n",
       " 'catholic church': 4,\n",
       " 'meet new': 4,\n",
       " 'million dollar': 4,\n",
       " 'would say': 4,\n",
       " 'jewish elite': 4,\n",
       " 'nancy pelosi': 4,\n",
       " 'de rothschild': 4,\n",
       " 'jeffrey epstein': 4,\n",
       " 'also need': 4,\n",
       " 'grand marshal': 4,\n",
       " 'american lunar': 4,\n",
       " 'crowning achievement': 4,\n",
       " 'took place': 4,\n",
       " 'donald trump': 4,\n",
       " 'financial system': 4,\n",
       " 'intelligence gathering': 4,\n",
       " 'joe biden': 4,\n",
       " 'ancient israelite': 4,\n",
       " 'cancer destroying': 4,\n",
       " 'cancer destroying middle': 4,\n",
       " 'destroying middle': 4,\n",
       " 'destroying middle east': 4,\n",
       " 'twin cancer': 4,\n",
       " 'twin cancer destroying': 4,\n",
       " 'national security': 4,\n",
       " 'social medium': 4,\n",
       " 'air traffic control': 4,\n",
       " 'traffic control': 4,\n",
       " 'american jewish': 4,\n",
       " 'christ cursed': 4,\n",
       " 'special military': 4,\n",
       " 'special military operation': 4,\n",
       " 'mike tyson': 3,\n",
       " 'fema camp': 3,\n",
       " 'cabal need': 3,\n",
       " 'zog zionist occupied': 3,\n",
       " 'pulled wef': 3,\n",
       " 'zog zionist': 3,\n",
       " 'fuck new': 3,\n",
       " 'fuck new world': 3,\n",
       " 'dont let': 3,\n",
       " 'queen story': 3,\n",
       " 'queen story time': 3,\n",
       " 'story time': 3,\n",
       " 'keep getting': 3,\n",
       " 'zionist jewish cabal': 3,\n",
       " 'deep state cabal': 3,\n",
       " 'state cabal': 3,\n",
       " 'jacob rothschild': 3,\n",
       " 'usa get': 3,\n",
       " 'elite killed unborn': 3,\n",
       " 'killed unborn': 3,\n",
       " 'killed unborn child': 3,\n",
       " 'unborn child': 3,\n",
       " 'color revolution': 3,\n",
       " 'jewish people': 3,\n",
       " 'agent embedded': 3,\n",
       " 'end game': 3,\n",
       " 'open society': 3,\n",
       " 'orthodox christian': 3,\n",
       " 'mental illness': 3,\n",
       " 'therefore america': 3,\n",
       " 'rejected christ': 3,\n",
       " 'german christian': 3,\n",
       " 'holy war': 3,\n",
       " 'across video': 3,\n",
       " 'across video must': 3,\n",
       " 'anti white genocide': 3,\n",
       " 'came across': 3,\n",
       " 'came across video': 3,\n",
       " 'video must': 3,\n",
       " 'video must watch': 3,\n",
       " 'destroy america': 3,\n",
       " 'jewish banker': 3,\n",
       " 'entry new': 3,\n",
       " 'entry new world': 3,\n",
       " 'idiot destroying': 3,\n",
       " 'idiot destroying entire': 3,\n",
       " 'must watch': 3,\n",
       " 'plain sight': 3,\n",
       " 'always going': 3,\n",
       " 'back day': 3,\n",
       " 'white genocide': 3,\n",
       " 'balfour declaration': 3,\n",
       " 'von braun': 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39181c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_terms=[]\n",
    "for key in imp_terms:\n",
    "    important_terms.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d3a6a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new world',\n",
       " 'world order',\n",
       " 'zionist occupied',\n",
       " 'united state',\n",
       " 'goy know',\n",
       " 'george soros',\n",
       " 'blood type',\n",
       " 'vatican ii',\n",
       " 'nostra aetate',\n",
       " 'occupied government',\n",
       " 'human right',\n",
       " 'jesus christ',\n",
       " 'deep state',\n",
       " 'zionist reptilian',\n",
       " 'far right',\n",
       " 'white people',\n",
       " 'jewish community',\n",
       " 'fascist white',\n",
       " 'new york',\n",
       " 'including zionist',\n",
       " 'church always',\n",
       " 'un special',\n",
       " 'late 20th',\n",
       " 'space time',\n",
       " 'space time information',\n",
       " 'time information',\n",
       " 'white power',\n",
       " 'african identity',\n",
       " 'zionist jewish',\n",
       " 'moon landing',\n",
       " 'middle east',\n",
       " 'critical race',\n",
       " 'church teaching',\n",
       " 'people color',\n",
       " 'civil right',\n",
       " 'world war',\n",
       " 'old school',\n",
       " 'zionist time',\n",
       " 'special rapporteur',\n",
       " 'un special rapporteur',\n",
       " 'federal reserve',\n",
       " 'white men',\n",
       " 'interfere behalf',\n",
       " 'say jew',\n",
       " 'throw jew',\n",
       " 'get rid',\n",
       " 'need get',\n",
       " 'zionist jew',\n",
       " 'klaus schwab',\n",
       " 'khazarian satanist',\n",
       " 'mel gibson',\n",
       " 'central bank',\n",
       " 'sound like',\n",
       " 'social justice',\n",
       " 'american jew',\n",
       " 'jewish lobby group',\n",
       " 'lobby group',\n",
       " 'published online',\n",
       " 'always taught',\n",
       " 'church always taught',\n",
       " 'communist party',\n",
       " 'ruling class',\n",
       " 'jew well',\n",
       " 'throw jew well',\n",
       " 'real jew',\n",
       " 'go back',\n",
       " 'khazarian mafia',\n",
       " 'white house',\n",
       " 'jewish cabal',\n",
       " 'national socialist',\n",
       " 'destroying nation',\n",
       " 'destroying entire',\n",
       " 'full disclosure',\n",
       " 'state america',\n",
       " 'united state america',\n",
       " 'military operation',\n",
       " 'year ago',\n",
       " 'air traffic',\n",
       " 'public service',\n",
       " 'white american',\n",
       " 'german people',\n",
       " '20th century',\n",
       " 'white race',\n",
       " 'jew accept',\n",
       " 'jew reject',\n",
       " 'organized jewry',\n",
       " 'roy cohn',\n",
       " 'elite killed',\n",
       " 'hollywood elite',\n",
       " 'hollywood elite killed',\n",
       " 'globalists cabal',\n",
       " 'world economic',\n",
       " 'accursed guilty',\n",
       " 'ethnic foids',\n",
       " 'manhattan new',\n",
       " 'manhattan new york',\n",
       " 'new york city',\n",
       " 'york city',\n",
       " 'cultural marxist',\n",
       " 'name zionist',\n",
       " 'blame slav',\n",
       " 'mayer amschel',\n",
       " 'political correctness',\n",
       " 'interest group',\n",
       " 'african identity black',\n",
       " 'identity black',\n",
       " 'frankfurt school',\n",
       " 'one listening',\n",
       " 'nazi germany',\n",
       " 'coup attempt',\n",
       " 'rothschild family',\n",
       " 'western civilization',\n",
       " 'chairman jewish',\n",
       " 'chairman jewish lobby',\n",
       " 'anti white',\n",
       " 'catholic church',\n",
       " 'meet new',\n",
       " 'million dollar',\n",
       " 'would say',\n",
       " 'jewish elite',\n",
       " 'nancy pelosi',\n",
       " 'de rothschild',\n",
       " 'jeffrey epstein',\n",
       " 'also need',\n",
       " 'grand marshal',\n",
       " 'american lunar',\n",
       " 'crowning achievement',\n",
       " 'took place',\n",
       " 'donald trump',\n",
       " 'financial system',\n",
       " 'intelligence gathering',\n",
       " 'joe biden',\n",
       " 'ancient israelite',\n",
       " 'cancer destroying',\n",
       " 'cancer destroying middle',\n",
       " 'destroying middle',\n",
       " 'destroying middle east',\n",
       " 'twin cancer',\n",
       " 'twin cancer destroying',\n",
       " 'national security',\n",
       " 'social medium',\n",
       " 'air traffic control',\n",
       " 'traffic control',\n",
       " 'american jewish',\n",
       " 'christ cursed',\n",
       " 'special military',\n",
       " 'special military operation',\n",
       " 'mike tyson',\n",
       " 'fema camp',\n",
       " 'cabal need',\n",
       " 'zog zionist occupied',\n",
       " 'pulled wef',\n",
       " 'zog zionist',\n",
       " 'fuck new',\n",
       " 'fuck new world',\n",
       " 'dont let',\n",
       " 'queen story',\n",
       " 'queen story time',\n",
       " 'story time',\n",
       " 'keep getting',\n",
       " 'zionist jewish cabal',\n",
       " 'deep state cabal',\n",
       " 'state cabal',\n",
       " 'jacob rothschild',\n",
       " 'usa get',\n",
       " 'elite killed unborn',\n",
       " 'killed unborn',\n",
       " 'killed unborn child',\n",
       " 'unborn child',\n",
       " 'color revolution',\n",
       " 'jewish people',\n",
       " 'agent embedded',\n",
       " 'end game',\n",
       " 'open society',\n",
       " 'orthodox christian',\n",
       " 'mental illness',\n",
       " 'therefore america',\n",
       " 'rejected christ',\n",
       " 'german christian',\n",
       " 'holy war',\n",
       " 'across video',\n",
       " 'across video must',\n",
       " 'anti white genocide',\n",
       " 'came across',\n",
       " 'came across video',\n",
       " 'video must',\n",
       " 'video must watch',\n",
       " 'destroy america',\n",
       " 'jewish banker',\n",
       " 'entry new',\n",
       " 'entry new world',\n",
       " 'idiot destroying',\n",
       " 'idiot destroying entire',\n",
       " 'must watch',\n",
       " 'plain sight',\n",
       " 'always going',\n",
       " 'back day',\n",
       " 'white genocide',\n",
       " 'balfour declaration',\n",
       " 'von braun']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66b8cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only consider original form of words.\n",
    "new_terms=[]\n",
    "\n",
    "for t in important_terms:\n",
    "    for text in uncleaned_texts:\n",
    "        if t in text:\n",
    "            new_terms.append(t)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "699a05e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78aa6405",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets first remove the appearence terms to make it emerging terms\n",
    "new_glossary=[]\n",
    "for term in glossary:\n",
    "    if len(term.split(\" \"))==1:\n",
    "        new_glossary.append(term)\n",
    "    elif len(term.split(\" \"))==2:\n",
    "        new_glossary.append(term.split()[0])\n",
    "        new_glossary.append(term.split()[1])\n",
    "    else:\n",
    "        terms=list(zip(term.split()[:-1],term.split()[1:]))\n",
    "        pairs=[' '.join(i)  for i in terms]\n",
    "        pairs.append(' '.join([term.split()[0],term.split()[-1]]))\n",
    "        new_glossary+=pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e6b0c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in new_glossary:\n",
    "    for t in new_terms:\n",
    "        if g in t:\n",
    "            new_terms.remove(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d261a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416e3e6",
   "metadata": {},
   "source": [
    "### Removing obvious jewish relevent terms to make it coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ef6b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jewish_topics=['kike','jew','zionist','nazi']\n",
    "for j in jewish_topics:\n",
    "    for t in new_terms:\n",
    "        if j in t:\n",
    "            new_terms.remove(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b685d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471e341",
   "metadata": {},
   "source": [
    "### Get the original form of the term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ea8c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams=[]\n",
    "for term in new_terms:\n",
    "    if len(term.split())==3:\n",
    "        trigrams.append(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "abcc3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing bigrams if they are already in trigrams\n",
    "for exp in trigrams:\n",
    "    terms=list(zip(exp.split()[:-1],exp.split()[1:]))\n",
    "    pairs=[' '.join(i)  for i in terms]\n",
    "    pairs.append(' '.join([exp.split()[0],exp.split()[-1]]))\n",
    "    if pairs[0] in new_terms:\n",
    "        new_terms.remove(pairs[0])\n",
    "    if pairs[1] in new_terms:\n",
    "        new_terms.remove(pairs[1])\n",
    "    if pairs[2] in new_terms:\n",
    "        new_terms.remove(pairs[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ede31c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bef3aec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united state',\n",
       " 'blood type',\n",
       " 'vatican ii',\n",
       " 'nostra aetate',\n",
       " 'human right',\n",
       " 'jesus christ',\n",
       " 'far right',\n",
       " 'white people',\n",
       " 'fascist white',\n",
       " 'late 20th',\n",
       " 'space time information',\n",
       " 'white power',\n",
       " 'moon landing',\n",
       " 'middle east',\n",
       " 'critical race',\n",
       " 'church teaching',\n",
       " 'civil right',\n",
       " 'world war',\n",
       " 'old school',\n",
       " 'un special rapporteur',\n",
       " 'federal reserve',\n",
       " 'white men',\n",
       " 'get rid',\n",
       " 'klaus schwab',\n",
       " 'khazarian satanist',\n",
       " 'mel gibson',\n",
       " 'central bank',\n",
       " 'sound like',\n",
       " 'social justice',\n",
       " 'church always taught',\n",
       " 'ruling class',\n",
       " 'go back',\n",
       " 'khazarian mafia',\n",
       " 'white house',\n",
       " 'national socialist',\n",
       " 'destroying nation',\n",
       " 'full disclosure',\n",
       " 'public service',\n",
       " 'white american',\n",
       " 'german people',\n",
       " '20th century',\n",
       " 'white race',\n",
       " 'roy cohn',\n",
       " 'world economic',\n",
       " 'ethnic foids',\n",
       " 'manhattan new york',\n",
       " 'new york city',\n",
       " 'mayer amschel',\n",
       " 'political correctness',\n",
       " 'interest group',\n",
       " 'african identity black',\n",
       " 'frankfurt school',\n",
       " 'coup attempt',\n",
       " 'western civilization',\n",
       " 'catholic church',\n",
       " 'million dollar',\n",
       " 'would say',\n",
       " 'nancy pelosi',\n",
       " 'jeffrey epstein',\n",
       " 'also need',\n",
       " 'grand marshal',\n",
       " 'american lunar',\n",
       " 'crowning achievement',\n",
       " 'took place',\n",
       " 'donald trump',\n",
       " 'financial system',\n",
       " 'intelligence gathering',\n",
       " 'joe biden',\n",
       " 'twin cancer',\n",
       " 'national security',\n",
       " 'air traffic control',\n",
       " 'special military operation',\n",
       " 'mike tyson',\n",
       " 'fema camp',\n",
       " 'dont let',\n",
       " 'queen story time',\n",
       " 'deep state cabal',\n",
       " 'usa get',\n",
       " 'unborn child',\n",
       " 'color revolution',\n",
       " 'end game',\n",
       " 'open society',\n",
       " 'orthodox christian',\n",
       " 'mental illness',\n",
       " 'therefore america',\n",
       " 'rejected christ',\n",
       " 'german christian',\n",
       " 'holy war',\n",
       " 'anti white genocide',\n",
       " 'came across',\n",
       " 'destroy america',\n",
       " 'idiot destroying entire',\n",
       " 'must watch',\n",
       " 'plain sight',\n",
       " 'always going',\n",
       " 'balfour declaration',\n",
       " 'von braun']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e100753",
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary=['cabal','cultural marxism','deicide','holocough','jewish capitalist','the goyim know',\n",
    "           'jewish communist','jewish lobby','new world order','rothschild', 'soros','zionist',\n",
    "         'zionist occupied government','not the real jews','jew down','cosmopolitan elite'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac709c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions=glossary+new_terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4cc5ab",
   "metadata": {},
   "source": [
    "## Extracting semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7040c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_context_words_bigram(sentence, target_bigram, num_words_before, num_words_after):\n",
    "    words = sentence.split()\n",
    "    context_words = []\n",
    "    if len(target_bigram.split())==4:\n",
    "        for i in range(len(words)-3):\n",
    "            fourgram=\" \".join([words[i],words[i+1],words[i+2],words[i+3]])\n",
    "            if fourgram==target_bigram:\n",
    "                start_index = max(0, i - num_words_before)\n",
    "                end_index = min(len(words), i + num_words_after + 4)\n",
    "                context_words.append(words[start_index:end_index])            \n",
    "    elif len(target_bigram.split())==3:\n",
    "        for i in range(len(words) - 2):  # Loop through pairs of consecutive words\n",
    "            trigram=\" \".join([words[i],words[i+1],words[i+2]])\n",
    "            if trigram == target_bigram: \n",
    "                start_index = max(0, i - num_words_before)\n",
    "                end_index = min(len(words), i + num_words_after + 3)\n",
    "                context_words.append(words[start_index:end_index])\n",
    "    else:\n",
    "        for i in range(len(words) - 1):  # Loop through pairs of consecutive words\n",
    "            bigram = \" \".join([words[i], words[i + 1]])\n",
    "            if bigram == target_bigram or words[i]==target_bigram:\n",
    "                start_index = max(0, i - num_words_before)\n",
    "                end_index = min(len(words), i + num_words_after + 2)\n",
    "                context_words.append(words[start_index:end_index])\n",
    "\n",
    "    return [\" \".join(i) for i in context_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb86fa32",
   "metadata": {},
   "source": [
    "### This is to extracting embeddings as words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cbec0991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3539 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "new_text_lematize=[]\n",
    "for text in uncleaned_lematised_text:\n",
    "    t=\"[CLS] \"+text+\" [SEP]\"\n",
    "    text_tokenize=tokenizer.tokenize(t)\n",
    "    if len(text_tokenize)>512:\n",
    "        chunks=len(text_tokenize)//512\n",
    "        chunk_size=len(text.split())//(chunks+1)\n",
    "        for i in range(chunks+1):\n",
    "            extracted_text=\" \".join(text.split(\" \")[i*chunk_size:(i+1)*chunk_size])\n",
    "            new_text_lematize.append(extracted_text)\n",
    "    else:\n",
    "        new_text_lematize.append(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37f45b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_text_lematize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d6e4bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_unclean=[]\n",
    "for text in uncleaned_texts:\n",
    "    t=\"[CLS] \"+text+\" [SEP]\"\n",
    "    text_tokenize=tokenizer.tokenize(t)\n",
    "    if len(text_tokenize)>512:\n",
    "        chunks=len(text_tokenize)//512\n",
    "        chunk_size=len(text.split())//(chunks+1)\n",
    "        for i in range(chunks+1):\n",
    "            extracted_text=\" \".join(text.split(\" \")[i*chunk_size:(i+1)*chunk_size])\n",
    "            new_text_unclean.append(extracted_text)\n",
    "    else:\n",
    "        new_text_unclean.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5ec54441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings considering complete sentence and use surrounding words for embeddings\n",
    "def extract_embeddings(Posts:list[str],Expression:list[str],Range:int):\n",
    "    embed_dict={i:[] for i in Expression}\n",
    "    for post in Posts:\n",
    "        t=\"[CLS] \"+post+\" [SEP]\"\n",
    "        text_tokenize=tokenizer.tokenize(t)\n",
    "        if len(text_tokenize)<512:\n",
    "            tensor_input_ids=torch.tensor([tokenizer.convert_tokens_to_ids(text_tokenize)])\n",
    "            tensor_segment_ids= torch.tensor([[1]*len(text_tokenize)])\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                outputs=model(tensor_input_ids,tensor_segment_ids)\n",
    "            token_embeddings = torch.squeeze(outputs[0])\n",
    "            for term in Expression:\n",
    "                if term in post:\n",
    "                    sent=extract_context_words_bigram(sentence=post,target_bigram=term,num_words_before=Range,num_words_after=Range)\n",
    "                    for t in sent:\n",
    "                        sent_tokenize=tokenizer.tokenize(t)\n",
    "                        token_index=[index for index,token in enumerate(text_tokenize) if token in sent_tokenize]\n",
    "                        sent_embed=[]\n",
    "                        for index in token_index:\n",
    "                            sent_embed.append(token_embeddings[index])\n",
    "                        if sent_embed:\n",
    "                            embed=torch.mean(torch.stack(sent_embed),dim=0)\n",
    "                            embed_dict[term].append(embed)\n",
    "    return embed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fd16119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "impembeddings=extract_embeddings(new_text_lematize,new_terms,Range=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de517056",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_embed_terms=[]\n",
    "for i in impembeddings:\n",
    "    if impembeddings[i]==[]:\n",
    "        no_embed_terms.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ae6d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "impterm_embeddings={}\n",
    "for term in new_terms:\n",
    "    if term not in no_embed_terms:\n",
    "        impterm_embeddings[term]=torch.mean(torch.stack(impembeddings[term]),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a97379d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_embeddings=extract_embeddings(new_text_unclean,glossary,Range=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66e41de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_embed_terms=[]\n",
    "for i in g_embeddings:\n",
    "    if g_embeddings[i]==[]:\n",
    "        no_embed_terms.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b4d77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_embeddings={}\n",
    "for term in glossary:\n",
    "    if term not in no_embed_terms:\n",
    "        glossary_embeddings[term]=torch.mean(torch.stack(g_embeddings[term]),dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433d477",
   "metadata": {},
   "source": [
    "#### To get sentence embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d88b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(Posts:list[str],Expression:list[str],rAnge:int):\n",
    "    terms_not_intext=[]\n",
    "    embed_dict={}\n",
    "    sent_dict={i:[] for i in Expression}\n",
    "    for term in Expression:\n",
    "        for post in Posts:\n",
    "            if term in post:\n",
    "                sents=extract_context_words_bigram(sentence=post,target_bigram=term,num_words_before=rAnge,num_words_after=rAnge)\n",
    "                if bool(sents):\n",
    "                    for sent in sents:\n",
    "                        sent_dict[term].append(sent)\n",
    "    for key in sent_dict:\n",
    "        if bool(sent_dict[key]):\n",
    "            sent_tokenize=tokenizer(sent_dict[key],return_tensors='pt',padding=True,max_length=512,truncation=True)\n",
    "            with torch.no_grad():\n",
    "                model_output=model(**sent_tokenize)['pooler_output']\n",
    "                embed_dict[key]=model_output\n",
    "        else:\n",
    "            terms_not_intext.append(key)\n",
    "        \n",
    "    return embed_dict,terms_not_intext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9308a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "impembeddings,no_embed_terms=extract_embeddings(new_text_lematize,new_terms,rAnge=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "5d8ab4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "impterm_embeddings={}\n",
    "for term in new_terms:\n",
    "    if term not in no_embed_terms:\n",
    "        impterm_embeddings[term]=torch.mean(impembeddings[term],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "id": "66f9fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_embeddings,no_embed_terms=extract_embeddings(new_text_unclean,glossary,rAnge=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "id": "e5958d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary_embeddings={}\n",
    "for term in glossary:\n",
    "    glossary_embeddings[term]=torch.mean(g_embeddings[term],dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f53ef0",
   "metadata": {},
   "source": [
    "### Fine tuning for word embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "id": "dfb35416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range = 6\n",
      "Range = 7\n",
      "Range = 8\n",
      "Range = 9\n",
      "Range = 10\n",
      "Range = 11\n",
      "Range = 12\n",
      "Range = 13\n",
      "Range = 14\n"
     ]
    }
   ],
   "source": [
    "predicted_labels={}\n",
    "column=[\"Term\"]+glossary+[\"Average\"]\n",
    "for r in range(5,15):\n",
    "    print(\"Range = \"+str(r))\n",
    "    impembeddings=extract_embeddings(new_text_lematize,new_terms,Range=r)\n",
    "    g_embeddings=extract_embeddings(new_text_unclean,glossary,Range=r)\n",
    "    impterm_embeddings={}\n",
    "    for term in new_terms:\n",
    "        impterm_embeddings[term]=torch.mean(torch.stack(impembeddings[term]),dim=0)\n",
    "    glossary_embeddings={}\n",
    "    for term in glossary:\n",
    "        glossary_embeddings[term]=torch.mean(torch.stack(g_embeddings[term]),dim=0)\n",
    "    bert_similarity_df=pd.DataFrame(columns=column)\n",
    "    for term in impterm_embeddings:\n",
    "        score=0\n",
    "        sim_score=[term]\n",
    "        for seed_word in glossary :\n",
    "            s=np.array(torch.cosine_similarity(impterm_embeddings[term].reshape(1,-1),glossary_embeddings[seed_word].reshape(1,-1)))[0]\n",
    "            sim_score.append(s)\n",
    "            score=score+s\n",
    "        sim_score.append(score/len(glossary))\n",
    "        bert_similarity_df.loc[len(bert_similarity_df)]=sim_score\n",
    "    threshold=bert_similarity_df['Average'].quantile(0.50)\n",
    "    bert_similarity_df['predicted']=bert_similarity_df['Average'].apply(lambda x:1 if x>threshold else 0)\n",
    "    predicted_labels[r]=bert_similarity_df['predicted']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b05e61",
   "metadata": {},
   "source": [
    "### Fine tuning for sentence embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "id": "ec3eddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range = 5\n",
      "Range = 6\n",
      "Range = 7\n",
      "Range = 8\n",
      "Range = 9\n",
      "Range = 10\n",
      "Range = 11\n",
      "Range = 12\n",
      "Range = 13\n",
      "Range = 14\n"
     ]
    }
   ],
   "source": [
    "predicted_labels={}\n",
    "column=[\"Term\"]+glossary+[\"Average\"]\n",
    "for r in range(5,15):\n",
    "    print(\"Range = \"+str(r))\n",
    "    impembeddings,no_term_embed=extract_embeddings(new_text_lematize,new_terms,rAnge=r)\n",
    "    g_embeddings,no_term_embed=extract_embeddings(new_text_unclean,glossary,rAnge=r)\n",
    "    impterm_embeddings={}\n",
    "    for term in new_terms:\n",
    "        impterm_embeddings[term]=torch.mean(impembeddings[term],dim=0)\n",
    "    glossary_embeddings={}\n",
    "    for term in glossary:\n",
    "        glossary_embeddings[term]=torch.mean(g_embeddings[term],dim=0)\n",
    "    bert_similarity_df=pd.DataFrame(columns=column)\n",
    "    for term in impterm_embeddings:\n",
    "        score=0\n",
    "        sim_score=[term]\n",
    "        for seed_word in glossary :\n",
    "            s=np.array(torch.cosine_similarity(impterm_embeddings[term].reshape(1,-1),glossary_embeddings[seed_word].reshape(1,-1)))[0]\n",
    "            sim_score.append(s)\n",
    "            score=score+s\n",
    "        sim_score.append(score/len(glossary))\n",
    "        bert_similarity_df.loc[len(bert_similarity_df)]=sim_score\n",
    "    threshold=bert_similarity_df['Average'].quantile(0.50)\n",
    "    bert_similarity_df['predicted']=bert_similarity_df['Average'].apply(lambda x:1 if x>threshold else 0)\n",
    "    predicted_labels[r]=bert_similarity_df['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "id": "fe5d4f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0     1\n",
       " 1     0\n",
       " 2     1\n",
       " 3     0\n",
       " 4     0\n",
       "      ..\n",
       " 89    0\n",
       " 90    1\n",
       " 91    1\n",
       " 92    0\n",
       " 93    1\n",
       " Name: predicted, Length: 94, dtype: int64,\n",
       " 6: 0     1\n",
       " 1     0\n",
       " 2     1\n",
       " 3     0\n",
       " 4     0\n",
       "      ..\n",
       " 89    0\n",
       " 90    0\n",
       " 91    1\n",
       " 92    0\n",
       " 93    1\n",
       " Name: predicted, Length: 94, dtype: int64,\n",
       " 7: 0     1\n",
       " 1     0\n",
       " 2     1\n",
       " 3     0\n",
       " 4     0\n",
       "      ..\n",
       " 89    0\n",
       " 90    0\n",
       " 91    1\n",
       " 92    0\n",
       " 93    1\n",
       " Name: predicted, Length: 94, dtype: int64,\n",
       " 8: 0     1\n",
       " 1     0\n",
       " 2     1\n",
       " 3     0\n",
       " 4     0\n",
       "      ..\n",
       " 89    0\n",
       " 90    0\n",
       " 91    1\n",
       " 92    0\n",
       " 93    1\n",
       " Name: predicted, Length: 94, dtype: int64,\n",
       " 9: 0     1\n",
       " 1     0\n",
       " 2     0\n",
       " 3     0\n",
       " 4     0\n",
       "      ..\n",
       " 89    0\n",
       " 90    0\n",
       " 91    1\n",
       " 92    0\n",
       " 93    1\n",
       " Name: predicted, Length: 94, dtype: int64,\n",
       " 10: 0     1\n",
       " 1     0\n",
       " 2     0\n",
       " 3     0\n",
       " 4     0\n",
       "      ..\n",
       " 89    0\n",
       " 90    0\n",
       " 91    1\n",
       " 92    0\n",
       " 93    1\n",
       " Name: predicted, Length: 94, dtype: int64,\n",
       " 11: 0     1\n",
       " 1     0\n",
       " 2     0\n",
       " 3     0\n",
       " 4     0\n",
       "      ..\n",
       " 89    0\n",
       " 90    0\n",
       " 91    1\n",
       " 92    0\n",
       " 93    1\n",
       " Name: predicted, Length: 94, dtype: int64,\n",
       " 12: 0     1\n",
       " 1     0\n",
       " 2     0\n",
       " 3     0\n",
       " 4     0\n",
       "      ..\n",
       " 89    0\n",
       " 90    0\n",
       " 91    1\n",
       " 92    0\n",
       " 93    1\n",
       " Name: predicted, Length: 94, dtype: int64,\n",
       " 13: 0     1\n",
       " 1     1\n",
       " 2     0\n",
       " 3     0\n",
       " 4     0\n",
       "      ..\n",
       " 89    0\n",
       " 90    0\n",
       " 91    1\n",
       " 92    0\n",
       " 93    1\n",
       " Name: predicted, Length: 94, dtype: int64,\n",
       " 14: 0     1\n",
       " 1     1\n",
       " 2     0\n",
       " 3     0\n",
       " 4     0\n",
       "      ..\n",
       " 89    0\n",
       " 90    0\n",
       " 91    1\n",
       " 92    0\n",
       " 93    1\n",
       " Name: predicted, Length: 94, dtype: int64}"
      ]
     },
     "execution_count": 1150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "id": "fc09c77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame.from_dict(predicted_labels)\n",
    "results['avg']=results.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "id": "d8526129",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Predicted']=results['avg'].apply(lambda x:1 if x>0.8 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "id": "4b8628c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "     ..\n",
       "89    0\n",
       "90    0\n",
       "91    1\n",
       "92    0\n",
       "93    0\n",
       "Name: Predicted, Length: 94, dtype: int64"
      ]
     },
     "execution_count": 969,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "id": "66b387b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv(\"Solution2-2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f46ca3",
   "metadata": {},
   "source": [
    "## Classification of 4 Approaches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b30732ef",
   "metadata": {},
   "source": [
    "##### Solution 1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d879d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e51c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=pd.read_csv(\"/Users/dhanushkikkisetti/Documents/Research Assistant/DSAA 2024/Solution 1-1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11971b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for : Solution 1-1\n",
      "-----------------------------------------\n",
      "Accuracy :  0.7884615384615384\n",
      "Precision :  0.3888888888888889\n",
      "Recall  :  1.0\n",
      "F1 Score  :  0.56\n"
     ]
    }
   ],
   "source": [
    "### 1 using Bert model to extract embedding using tokens to compare similarity between imp terms and glossary\n",
    "#data_file=pd.read_csv(\"/Users/dhanushkikkisetti/Documents/Research Assistant/Scripts/1.csv\",encoding='latin-1')\n",
    "print(\"Results for : Solution 1-1\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Accuracy : \",accuracy_score(data_file['Actual'],data_file['Predicted']))\n",
    "print(\"Precision : \",precision_score(data_file['Actual'],data_file['Predicted']))\n",
    "print(\"Recall  : \",recall_score(data_file['Actual'],data_file['Predicted']))\n",
    "print(\"F1 Score  : \",f1_score(data_file['Actual'],data_file['Predicted']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52e835d0",
   "metadata": {},
   "source": [
    "##### Solution 1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf98b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=pd.read_csv(\"/Users/dhanushkikkisetti/Documents/Research Assistant/DSAA 2024/Solution 1-2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2227c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for : Solution 1-2\n",
      "-----------------------------------------\n",
      "Accuracy :  0.7692307692307693\n",
      "Precision :  1.0\n",
      "Recall  :  0.3684210526315789\n",
      "F1 Score  :  0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "print(\"Results for : Solution 1-2\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Accuracy : \",accuracy_score(data_file['Predicted'],data_file['Actual']))\n",
    "print(\"Precision : \",precision_score(data_file['Predicted'],data_file['Actual']))\n",
    "print(\"Recall  : \",recall_score(data_file['Predicted'],data_file['Actual']))\n",
    "print(\"F1 Score  : \",f1_score(data_file['Predicted'],data_file['Actual']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c5fddad",
   "metadata": {},
   "source": [
    "##### Solution 2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c49c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=pd.read_csv(\"/Users/dhanushkikkisetti/Documents/Research Assistant/DSAA 2024/Solution 2-1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324bce5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for : Solution 2-1\n",
      "-----------------------------------------\n",
      "Accuracy :  0.6808510638297872\n",
      "Precision :  0.4857142857142857\n",
      "Recall  :  0.5862068965517241\n",
      "F1 Score  :  0.53125\n"
     ]
    }
   ],
   "source": [
    "print(\"Results for : Solution 2-1\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Accuracy : \",accuracy_score(data_file['Actual'],data_file['Predicted']))\n",
    "print(\"Precision : \",precision_score(data_file['Actual'],data_file['Predicted']))\n",
    "print(\"Recall  : \",recall_score(data_file['Actual'],data_file['Predicted']))\n",
    "print(\"F1 Score  : \",f1_score(data_file['Actual'],data_file['Predicted']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64f6edcc",
   "metadata": {},
   "source": [
    "##### solution 2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e6074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=pd.read_csv(\"/Users/dhanushkikkisetti/Documents/Research Assistant/DSAA 2024/Solution 2-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3237725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for : Solution 2-2\n",
      "-----------------------------------------\n",
      "Accuracy :  0.7978723404255319\n",
      "Precision :  0.631578947368421\n",
      "Recall  :  0.8275862068965517\n",
      "F1 Score  :  0.716417910447761\n"
     ]
    }
   ],
   "source": [
    "print(\"Results for : Solution 2-2\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"Accuracy : \",accuracy_score(data_file['Actual'],data_file['Predicted']))\n",
    "print(\"Precision : \",precision_score(data_file['Actual'],data_file['Predicted']))\n",
    "print(\"Recall  : \",recall_score(data_file['Actual'],data_file['Predicted']))\n",
    "print(\"F1 Score  : \",f1_score(data_file['Actual'],data_file['Predicted']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
